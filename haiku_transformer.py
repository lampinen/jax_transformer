import haiku as hk
import jax
import jax.numpy as jnp
import datasets


def softmax_xe(logits, labels):
    return -jnp.sum(jax.nn.log_softmax(logits) * labels, axis=-1)


class SingleAttentionHead(hk.Module):
    def __init__(self, dimensionality, name=None):
        super(SingleAttentionHead, self).__init__(name=name)
        self.dimensionality = dimensionality

    def __call__(self, inputs, Q_inputs=None):
        dimensionality = self.dimensionality
        # are queries generated by a different set of inputs? (for output stack)
        if Q_inputs is None:
            Q_inputs = inputs

        Q = hk.Linear(dimensionality)(Q_inputs) 
        K = hk.Linear(dimensionality)(inputs) 
        V = hk.Linear(dimensionality)(inputs)

        attention_scale = jnp.sqrt(dimensionality)
        
        attention_weights = jnp.matmul(
            Q, jnp.transpose(K, axes=[0,2,1])) / attention_scale
            
        attention_weights = jax.nn.softmax(attention_weights, axis=-1)

        outputs = jnp.matmul(attention_weights, V)

        return outputs

class AndrewsTransformer(hk.Module):
    def __init__(self, dimensionality, name=None):
        super(AndrewsTransformer, self).__init__(name=name)
        self.dimensionality = dimensionality

    def __call__(self, inputs):
        dimensionality = self.dimensionality
        one_hot = jax.nn.one_hot(inputs, num_classes=num_ints)

        embedding = hk.Linear(dimensionality)
        embedded_inputs = embedding(one_hot)

        # testing, replace
        att1 = SingleAttentionHead(dimensionality)
        results = att1(embedded_inputs)
        return results

if __name__ == "__main__":
    #### Params
    num_ints = 11
    seq_length = 7
    num_train = 1000
    num_test = 100
    dimensionality = 128
    #### 

    rng = jax.random.PRNGKey(0)
    dataset = datasets.build_reversing_dataset(
        num_train=num_train,
        num_test=num_test,
        num_ints=num_ints,
        seq_length=seq_length)

    batch_inputs = dataset["train"]["inputs"][:5]

    def model_forward_fn(inputs):
        model = AndrewsTransformer(dimensionality=dimensionality) 
        return model(inputs)

    forward = hk.transform(model_forward_fn)
    params = forward.init(rng, batch_inputs)
    results = forward.apply(params, None, batch_inputs)
    print(results)
    print(results.shape)
    
